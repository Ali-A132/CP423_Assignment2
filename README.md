# CP423 Assignment #2

This assignment utilized many concepts we touched on in class around text preprocessing and tokenization, positional ranking of terms according to their location, document ranking using TF-IDF and cosine similarity between vectors. Since we are given such a large data set corpus with this assignment with 236 documents that contains a lot of data to sift through. We are going to examine this particular folder along with some custom queries I will provide to see if we can find matching documents along with creating matrices with the use of numpy.

### **Part One: Positional Index**

To start, we already have the preprocess function from assignment 1 which we can reuse since it removes stopwords, converts words to lowercase, removes punctuation, and blank tokens. For the positional index, I decided I would put all the methods into a class fitting the name. I initialized the index with a nested dictionary of lists since we are storing the term itself, the file name, along with the list of positions it is found to be in. Typically we would store the document id but instead I stored the name instead for the demonstration since it is easier to verify where a term or phrase is found. 

Next we can create our positional index with a function where we provide the folder in which our corpus is found. We can iterate through the file names and open and read each one to append it to a list called documents, if we can’t read a file, we can ignore it and move forward since some aren't readable through text. Next we can iterate through all of the terms that we preprocessed and iterate through them to populate our index with the position at each term found in a file. The Enumerate function allows us to index each value where we can keep count of each subsequent term which is why I used it. 

Now that we created our positional index, we can implement the phraseQuery processingFunction that takes a phrase given by a user and searches to find matching docs. With our function here, we take a phrase from a user and preprocess it which tokenizes it. We then verify if there exists less than 5 words, it should be noted that stop words don’t count which I can demonstrate in the example following soon. Next we can create a list where we append the indexed values where those terms are found which also contains their file location. With this info, we can find the intersection of all terms and save it to a common variable of all documents that contains all terms found in the phrase given by a user. 

Finally we iterate through all these found intersecting documents and we create a list of sets called positions where we store places where the term we are iterating on is found. We then iterate through this new list and find out if the subsequent terms following it in the exact order and are matching the phrase given by the user. If no match is found, we break out of the loop and our matching docs array will be empty. If there is a match for each upcoming term from the first, we append it to the array matchingDocs. It should be noted the reason why we are iterating at positions at 0 is because we are searching every place the first term is the phrase is found and seeing if all terms following it match our phrase within the doc. This function then returns all the matching documents no matter if it is empty or contains values.

### **Part Two: Ranking & Term Weighting - Part A: TF-IDF Matrix**

Part A: TF-IDF Matrix

For part 2, I will implement the tf-idf matrix for all 5 weight schemes provided. I started off with initializing our values of the given documents and terms we will be using along with a value to store term names for later. With this we can move on to our createTfIdf function that stores the length of the docs given and iterates through them. With the weight scheme, we need the term frequency values of how many times it appears, along with the max term frequency value and the total number of terms found in the doc. 

We can then iterate through all term names and find the raw count at each word. Depending on what weight we are on, we either find the binary, raw count, normalized tf, log normalization or double normalization with their appropriate calculation provided. After we compute the TF, we then find the document frequency, inverse document frequency and multiply both the term frequency and the idf value and store in the row and subsequently the matrix once we go through all terms in a doc. Using numpy, we can return the matrix formatted. When getting a query and processing it, the code is nearly the same but we aren’t iterating through docs which makes this simpler. 

Part B: Cosine Similarity NOT SOLVED

Finally we have the cosine similarity value that takes 2 vectors and finds the similarity of both, particularly being the query vector and document vector. This can be done through getting the cosine similarity of both the query and the document vector and we can find this through dot product. I could not solve this through python but I did establish this function to be similar at each point in the matrix, another function should call this to rank all the highest values for each pair. 

We can discuss the advantages and disadvantages of each term frequency weighting scheme. Binary seems to be the most simple with either using 1 or 0 but it may lose some nuance not ignoring term repetitions and does not normalize which may hurt our final results. The raw count simply captures the number of times a term appears in a doc which is also simple and easy to compute but it may be difficult in longer documents since there are so many terms. Normalized TF and log normalization normalizes document length but may be biased towards terms that appear a lot. Double normalization balances term frequency and document frequency but may be costly since we have to find the TF and max TF at every document level, despite this disadvantage I believe it works best with this data set due to how it handles these values and dampens the effect of terms that appear a lot.

That concludes my video explanation of this assignment, thank you for listening.
